# 大数据离线阶段 第三天

## HDFS 基本操作

### FileSystem Shell

Hadoop 提供了文件系统的 shell 命令行客户端，使用方法如下：

```shell
hadoop fs <args>
```

文件系统 shell 包括与 Hadoop 分布式文件系统（HDFS）以及 Hadoop 支持的其他文件系统（如本地 FS，HFTP FS，S3 FS 等）直接交互的各种类似 shell的命令。所有 FS shell 命令都将路径 URI 作为参数。

URI 格式为 scheme://authority/path。对于 HDFS，该 scheme 是 hdfs，对于本地 FS，该 scheme 是 file。scheme 和 authority 是可选的。如果未指定，则使用配置中指定的默认方案。

对于 HDFS,命令示例如下：

```shell
hadoop fs -ls  hdfs://namenode:host/parent/child
hadoop fs -ls  /parent/child 
#fs.defaultFS 中有配置
```

对于本地文件系统，命令示例如下：

```shell
hadoop fs -ls file:///root/
```

如果使用的文件系统是 HDFS，则使用 hdfs dfs 也是可以的，此时

```shell
hadoop fs <args> = hdfs dfs <args>
```

### Shell 命令选项



###  常用命令介绍

```shell
- ls
```

使用方法

```shell
hadoop fs -ls [-h] [-R] <args>
```

功能：显示文件、目录信息。

示例

```shell
hadoop fs -ls /user/hadoop/file1
```

```shell
-mkdir
```

使用方法

```shell
hadoop fs -mkdir [-p] <paths>
```

功能：在 hdfs 上创建目录，-p 表示会创建路径中的各级父目录。

示例

```shell
hadoop fs -mkdir –p /user/hadoop/dir1
```

```shell
- put
```

使用方法：

```shell
hadoop fs -put [-f] [-p] [ -|<localsrc1> .. ]. <dst>
```

功能：将单个 src 或多个 srcs 从本地文件系统复制到目标文件系统。

```shell
-p：保留访问和修改时间，所有权和权限。
-f：覆盖目的地（如果已经存在）
```

示例

```shell
hadoop fs -put -f localfile1 localfile2 /user/hadoop/hadoopdir
```

```shell
-get
```

使用方法

```shell
hadoop fs -get [-ignorecrc] [-crc] [-p] [-f] <src> <localdst>
#-ignorecrc：跳过对下载文件的 CRC 检查。
#-crc：为下载的文件写 CRC 校验和。
```

功能：将文件复制到本地文件系统。

示例

```shell
hadoop fs -get hdfs://host:port/user/hadoop/file localfile
```

```shell
-appendIoFile
```

使用方法

```shell
hadoop fs -appendToFile <localsrc> ... <dst>
```

功能：追加一个文件到已经存在的文件末尾

示例

```shell
hadoop fs -appendToFile localfile /hadoop/hadoopfil
```

```shell
-cat
```

使用方法

```shell
hadoop fs -cat [-ignoreCrc] URI [URI ...]
```

功能:显示文件内容到 stdout

示例

```shell
hadoop fs -cat /hadoop/hadoopfile
```

```shell
-tail
```

使用方法

```shell
hadoop fs -tail [-f] URI
```

功能：将文件的最后一千字节内容显示到 stdout。

-f 选项将在文件增长时输出附加数据。

示例

```shell
hadoop fs -tail /hadoop/hadoopfile
```

```shell
-chgrp
```

使用方法

```shell
hadoop fs -chgrp [-R] GROUP URI [URI ...]
```

功能：更改文件组的关联。用户必须是文件的所有者，否则是超级用户。

-R 将使改变在目录结构下递归进行。

示例

```shell
hadoop fs -chgrp othergroup /hadoop/hadoopfile
```

```shell
-chmod
```

功能：改变文件的权限。使用-R 将使改变在目录结构下递归进行。

示例

```shell
hadoop fs -chmod 666 /hadoop/hadoopfile
```

```shell
-chown
```

功能：改变文件的拥有者。使用-R 将使改变在目录结构下递归进行。

示例

```shell
hadoop fs -chown someuser:somegrp /hadoop/hadoopfile
```

```shell
-copyFromLocal
```

```shell
hadoop fs -copyFromLocal <localsrc> URI
```

功能：从本地文件系统中拷贝文件到 hdfs 路径去

```shell
示例：hadoop fs -copyFromLocal /root/1.txt /
```

```shell
-copyToLocal
```

功能：从 hdfs 拷贝到本地

示例

```shell
hadoop fs -copyToLocal /aaa/jdk.tar.gz
```

```shell
-cp
```

功能：从 hdfs 的一个路径拷贝 hdfs 的另一个路径

```shell
hadoop fs -cp /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2
```

```shell
-mv
```

功能：在 hdfs 目录中移动文件

```shell
hadoop fs -mv /aaa/jdk.tar.gz /
```

```shell
-getmerge
```

功能：合并下载多个文件
示例：比如 hdfs 的目录 /aaa/下有多个文件:log.1, log.2,log.3,...

```shell
hadoop fs -getmerge /aaa/log.* ./log.sum
```

```shell
-rm
```

功能：删除指定的文件。只删除非空目录和文件。-r 递归删除。

```shell
hadoop fs -rm -r /aaa/bbb/
```

```shell
-df
```

功能：统计文件系统的可用空间信息

示例

```shell
hadoop fs -df -h /
```

```shell
-du
```

功能：显示目录中所有文件大小，当只指定一个文件时，显示此文件的大小

示例 

```shell
hadoop fs -du /user/hadoop/dir1
```

```shell
-setrep
```

功能：改变一个文件的副本系数。-R 选项用于递归改变目录下所有文件的副本系数。

示例

```shell
hadoop fs -setrep -w 3 -R /user/hadoop/dir1
```

## HDFS  基本原理

### NameNode  概述

- NameNode 是 HDFS 的核心。
- NameNode 也称为 Master。
- NameNode 仅存储 HDFS 的元数据：文件系统中所有文件的目录树，并跟踪整个集群中的文件。
- NameNode 不存储实际数据或数据集。数据本身实际存储在 DataNodes 中。
- NameNode 知道 HDFS 中任何给定文件的块列表及其位置。使用此信息NameNode 知道如何从块中构建文件。
- NameNode 并不持久化存储每个文件中各个块所在的 DataNode 的位置信息，这些信息会在系统启动时从数据节点重建。
- NameNode 对于 HDFS 至关重要，当 NameNode 关闭时，HDFS / Hadoop 集群无法访问。
- NameNode 是 Hadoop 集群中的单点故障。
- NameNode 所在机器通常会配置有大量内存（RAM）。

![52938775354](H:\大数据班hadoop阶段\hadoop阶段笔记\day_05\images\1529387753542.png)

### DataNode  概述

- DataNode 负责将实际数据存储在 HDFS 中。
- DataNode 也称为 Slave。
- NameNode 和 DataNode 会保持不断通信。
- DataNode 启动时，它将自己发布到 NameNode 并汇报自己负责持有的块列表。
- 当某个 DataNode 关闭时，它不会影响数据或群集的可用性。NameNode 将安排由其他 DataNode 管理的块进行副本复制。
- DataNode 所在机器通常配置有大量的硬盘空间。因为实际数据存储在DataNode 中。
- DataNode 会定期（dfs.heartbeat.interval 配置项配置，默认是 3 秒）向NameNode 发送心跳，如果 NameNode 长时间没有接受到 DataNode 发送的心跳， NameNode 就会认为该 DataNode 失效。
- block 汇报时间间隔取参数 dfs.blockreport.intervalMsec,参数未配置的话默认为 6 小时.

### HDFS  的工作机制

NameNode 负责管理整个文件系统元数据；DataNode 负责管理具体文件数据块存储；Secondary NameNode 协助 NameNode 进行元数据的备份。
HDFS 的内部工作机制对客户端保持透明，客户端请求访问 HDFS 都是通过向NameNode 申请来进行。

![52938784526](H:\大数据班hadoop阶段\hadoop阶段笔记\day_05\images\1529387845264.png)



#### HDFS  写数据流程

**详细步骤解析**：

1. client 发起文件上传请求，通过 RPC 与 NameNode 建立通讯，NameNode检查目标文件是否已存在，父目录是否存在，返回是否可以上传；
2. client 请求第一个 block 该传输到哪些 DataNode 服务器上；
3. NameNode 根据配置文件中指定的备份数量及机架感知原理进行文件分配，返回可用的 DataNode 的地址如：A，B，C；注：Hadoop 在设计时考虑到数据的安全与高效，数据文件默认在 HDFS 上存放三份，存储策略为本地一份，同机架内其它某一节点上一份，不同机架的某一节点上一份。
4. client 请求 3 台 DataNode 中的一台 A 上传数据（本质上是一个 RPC 调用，建立 pipeline），A 收到请求会继续调用 B，然后 B 调用 C，将整个pipeline 建立完成，后逐级返回 client；
5. client 开始往 A 上传第一个 block（先从磁盘读取数据放到一个本地内存缓存），以 packet 为单位（默认 64K），A 收到一个 packet 就会传给 B，B 传给 C；A 每传一个 packet 会放入一个应答队列等待应答。
6. 数据被分割成一个个 packet 数据包在 pipeline 上依次传输，在pipeline 反方向上，逐个发送 ack（命令正确应答），最终由 pipeline中第一个 DataNode 节点 A 将 pipeline ack 发送给 client;
7.  当一个 block 传输完成之后，client 再次请求 NameNode 上传第二个block 到服务器。

#### HDFS  读数据流程

**详细步骤解析**

1. Client 向 NameNode 发起 RPC 请求，来确定请求文件 block 所在的位置；
2. NameNode会视情况返回文件的部分或者全部block列表，对于每个block，NameNode 都会返回含有该 block 副本的 DataNode 地址；
3.  这些返回的 DN 地址，会按照集群拓扑结构得出 DataNode 与客户端的距离，然后进行排序，排序两个规则：网络拓扑结构中距离 Client 近的排靠前；心跳机制中超时汇报的 DN 状态为 STALE，这样的排靠后；
4. Client 选取排序靠前的 DataNode 来读取 block，如果客户端本身就是DataNode,那么将从本地直接获取数据；
5. 底层上本质是建立 Socket Stream（FSDataInputStream），重复的调用父类 DataInputStream 的 read 方法，直到这个块上的数据读取完毕；
6. 当读完列表的 block 后，若文件读取还没有结束，客户端会继续向NameNode 获取下一批的 block 列表；
7. 读取完一个 block 都会进行 checksum 验证，如果读取 DataNode 时出现错误，客户端会通知 NameNode，然后再从下一个拥有该 block 副本的DataNode 继续读。
8. read 方法是并行的读取 block 信息，不是一块一块的读取；NameNode 只是返回Client请求包含块的DataNode地址，并不是返回请求块的数据；
9. 最终读取来所有的 block 会合并成一个完整的最终文件。

## HDFS  的应用开发

### HDFS 的 的 JAVA API  操作

HDFS 在生产应用中主要是客户端的开发，其核心步骤是从 HDFS 提供的 api中构造一个 HDFS 的访问客户端对象，然后通过该客户端对象操作（增删改查）HDFS 上的文件

#### 搭建开发环境

创建 Maven 工程，引入 pom 依赖

```xml
<dependencies>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-common</artifactId>
        <version>2.7.4</version>
    </dependency>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-hdfs</artifactId>
        <version>2.7.4</version>
    </dependency>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>2.7.4</version>
    </dependency>
</dependencies>
```

####  构造 客户端对象

在 java 中操作 HDFS，主要涉及以下 Class：
Configuration：该类的对象封转了客户端或者服务器的配置;
FileSystem：该类的对象是一个文件系统对象，可以用该对象的一些方法来对文件进行操作，通过 FileSystem 的静态方法 get 获得该对象。

```java
FileSystem fs = FileSystem.get(conf)
```

get 方法从 conf 中的一个参数 fs.defaultFS 的配置值判断具体是什么类型的文件系统。如果我们的代码中没有指定 fs.defaultFS，并且工程 classpath下也没有给定相应的配置，conf中的默认值就来自于hadoop的jar包中的core-default.xml ， 默 认 值 为 ： file:/// ， 则 获 取 的 将 不 是 一 个DistributedFileSystem 的实例，而是一个本地文件系统的客户端对象。

#### 示例代码

**框架封装的 API 操作**

```java
Configuration conf = new Configuration();
//这里指定使用的是 hdfs 文件系统
conf.set("fs.defaultFS", "hdfs://node-21:9000");
//通过如下的方式进行客户端身份的设置
System.setProperty("HADOOP_USER_NAME", "root");
//通过 FileSystem 的静态方法获取文件系统客户端对象
FileSystem fs = FileSystem.get(conf);
//也可以通过如下的方式去指定文件系统的类型 并且同时设置用户身份
//FileSystem fs = FileSystem.get(new URI("hdfs://node-21:9000"), conf, "root");
//列出 hdfs 根目录下的所有文件信息
RemoteIterator<LocatedFileStatus> listFiles = fs.listFiles(new Path("/"), false);
//遍历出我们得到的指定文件路径的迭代器 获取相应的文件信息
while (listFiles.hasNext()) {
    LocatedFileStatus fileStatus = listFiles.next();
    String name = fileStatus.getPath().getName();
    System.out.println(name);
}
//关闭我们的文件系统
fs.close();
```

其他更多操作如文件增删改查请查看实例代码github。

**Stream 流形式操作**

```java
FileSystem fs = null;
Configuration conf = null;
@Before
public void init() throws Exception{
conf = new Configuration();
FileSystem fs = FileSystem.get(new URI("hdfs://node-21:9000"), conf, "root");
}
/**
* 通过流的方式上传文件到 hdfs
* @throws Exception
*/
@Test
public void testUpload() throws Exception {
FSDataOutputStream outputStream = fs.create(new Path("/1.txt"), true);
FileInputStream inputStream = new FileInputStream("D:\\1.txt");
IOUtils.copy(inputStream, outputStream);
}
```

### 案例：shell  定时 采集至 数据至 HDFS

上线的网站每天都会产生日志数据。假如有这样的需求：要求在凌晨 24 点
开始操作前一天产生的日志文件，准实时上传至 HDFS 集群上。该如何实现？实现后能否实现周期性上传需求？如何定时？

#### 技术分析

```shell
HDFS SHELL:
hadoop fs –put #满足上传文件，不能满足定时、周期性传入。
#==========================================================
L L inux crontab: :
crontab -e
0 0 * * * /shell/ uploadFile2Hdfs.sh #每天凌晨 12：00 执行一次
```

#### 实现流程

一般日志文件生成的逻辑由业务系统决定，比如每小时滚动一次，或者一定大小滚动一次，避免单个日志文件过大不方便操作。
比如滚动后的文件命名为 access.log.x,其中 x 为数字。正在进行写的日志文件叫做 access.log。这样的话，如果日志文件后缀是 1\2\3 等数字，则该文件满足需求可以上传，就把该文件移动到准备上传的工作区间目录。工作区间有文件之后，可以使用 hadoop put 命令将文件上传。

####  代码实现

```shell
#!/bin/bash

#set java env
export JAVA_HOME=/root/apps/jdk1.8.0_65
export JRE_HOME=${JAVA_HOME}/jre
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
export PATH=${JAVA_HOME}/bin:$PATH

#set hadoop env
export HADOOP_HOME=/root/apps/hadoop-2.7.4
export PATH=${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:$PATH


#日志文件存放的目录
log_src_dir=/root/logs/log/

#待上传文件存放的目录
log_toupload_dir=/root/logs/toupload/


#日志文件上传到hdfs的根路径
date1=`date -d last-day +%Y_%m_%d`
hdfs_root_dir=/data/clickLog/$date1/ 

#打印环境变量信息
echo "envs: hadoop_home: $HADOOP_HOME"


#读取日志文件的目录，判断是否有需要上传的文件
echo "log_src_dir:"$log_src_dir
ls $log_src_dir | while read fileName
do
	if [[ "$fileName" == access.log.* ]]; then
	# if [ "access.log" = "$fileName" ];then
		date=`date +%Y_%m_%d_%H_%M_%S`
		#将文件移动到待上传目录并重命名
		#打印信息
		echo "moving $log_src_dir$fileName to $log_toupload_dir"xxxxx_click_log_$fileName"$date"
		mv $log_src_dir$fileName $log_toupload_dir"xxxxx_click_log_$fileName"$date
		#将待上传的文件path写入一个列表文件willDoing
		echo $log_toupload_dir"xxxxx_click_log_$fileName"$date >> $log_toupload_dir"willDoing."$date
	fi
	
done
#找到列表文件willDoing
ls $log_toupload_dir | grep will |grep -v "_COPY_" | grep -v "_DONE_" | while read line
do
	#打印信息
	echo "toupload is in file:"$line
	#将待上传文件列表willDoing改名为willDoing_COPY_
	mv $log_toupload_dir$line $log_toupload_dir$line"_COPY_"
	#读列表文件willDoing_COPY_的内容（一个一个的待上传文件名）  ,此处的line 就是列表中的一个待上传文件的path
	cat $log_toupload_dir$line"_COPY_" |while read line
	do
		#打印信息
		echo "puting...$line to hdfs path.....$hdfs_root_dir"
		hadoop fs -mkdir -p $hdfs_root_dir
		hadoop fs -put $line $hdfs_root_dir
	done	
	mv $log_toupload_dir$line"_COPY_"  $log_toupload_dir$line"_DONE_"
done

```

## 初识 MapReduce

### MapReduce计算模型介绍

#### 理解 MapReduce 

MapReduce 思想在生活中处处可见。即使是发布过论文《Google-File-System》的谷歌也只是实现了这种思想，而不是自己原创。简单来说，MapReduce的思想就是“ 分而治之”，特别适用于大量复杂的任务处理场景。

Map 负责“分”，即把复杂的任务分解为若干个“简单的任务”来处理。“简单的任务”包含三层含义：一是数据或计算的规模相对任务要大大缩小；二是就近计算原则，即任务会分配到存放着所需数据的节点上进行计算；三是这些小任务可以并行计算，彼此间几乎没有依赖关系。

Reduce 负责对 map 阶段的结果进行汇总。

这两个阶段合起来正是 MapReduce 思想的体现。生活工作中，我们或多或少都曾接触过这种思想。你能想起哪些呢？

![52938882660](H:\大数据班hadoop阶段\hadoop阶段笔记\day_05\images\1529388826608.png)



一个比较形象的语言解释 MapReduce：
我们要数图书馆中的所有书。你数1号书架，我数2号书架。这就是“ Map”。我们人越多，数书就更快。
现在我们到一起，把所有人的统计数加在一起。这就是“ Reduce”。

#### Hadoop MapReduce  设计构思

MapReduce 是一个分布式运算程序的编程框架，MapReduce 核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个 Hadoop 集群上。

既然是做计算的框架，那么表现形式就是有个输入（input），MapReduce 操作这个输入（input），通过本身定义好的计算模型，得到一个输出（output）。

对许多开发者来说，自己完完全全实现一个并行计算程序难度太大，而MapReduce 就是一种简化并行计算的编程模型，降低了开发并行应用的入门门槛。

Hadoop MapReduce 构思体现在如下的三个方面：

- 如何对付大数据处理：分而治之

  对相互间不具有计算依赖关系的大数据，实现并行最自然的办法就是采取分而治之的策略。并行计算的第一个重要问题是如何划分计算任务或者计算数据以便对划分的子任务或数据块同时进行计算。不可分拆的计算任务或相互间有依赖关系的数据无法进行并行计算！

- 构建抽象模型：Map 和 Reduce

  MapReduce 借鉴了函数式语言中的思想，用 Map 和 Reduce 两个函数提供了高层的并行编程抽象模型。
  Map: 对一组数据元素进行某种重复式的处理；
  Reduce: 对 Map 的中间结果进行某种进一步的结果整理。
  MapReduce 中定义了如下的 Map 和 Reduce 两个抽象的编程接口，由用户去编程实现:
  map: (k1; v1) → [(k2; v2)]
  reduce: (k2; [v2]) → [(k3; v3)]
  Map 和 Reduce 为程序员提供了一个清晰的操作接口抽象描述。通过以上两个编程接口，大家可以看出 MapReduce 处理的数据类型是<key,value>键值对。

- 统一构架，隐藏系统层细节

  如何提供统一的计算框架，如果没有统一封装底层细节，那么程序员则需要考虑诸如数据存储、划分、分发、结果收集、错误恢复等诸多细节；为此，MapReduce 设计并提供了统一的计算框架，为程序员隐藏了绝大多数系统层面的处理细节。

MapReduce 最大的亮点在于通过抽象模型和计算框架把需要做什么(whatneed to do)与具体怎么做(how to do)分开了，为程序员提供一个抽象和高层的编程接口和框架。程序员仅需要关心其应用层的具体计算问题，仅需编写少量的处理应用本身计算问题的程序代码。如何具体完成这个并行计算任务所相关的诸多系统层细节被隐藏起来,交给计算框架去处理：从分布代码的执行，到大到数千小到单个节点集群的自动调度使用。

#### MapReduce  框架结构

一个完整的 mapreduce 程序在分布式运行时有三类实例进程：

- MRAppMaster：负责整个程序的过程调度及状态协调
- MapTask：负责 map 阶段的整个数据处理流程
- ReduceTask：负责 reduce 阶段的整个数据处理流程



![52938909104](H:\大数据班hadoop阶段\hadoop阶段笔记\day_05\images\1529389091048.png)

### MapReduce  编程规范及示例编写

#### 编程规范

- 用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行 mr 程序的客户端)


- Mapper 的输入数据是 KV 对的形式（KV 的类型可自定义）
- Mapper 的输出数据是 KV 对的形式（KV 的类型可自定义）
- Mapper 中的业务逻辑写在 map()方法中
- map()方法（maptask 进程）对每一个<K,V>调用一次
- Reducer 的输入数据类型对应 Mapper 的输出数据类型，也是 KV
- Reducer 的业务逻辑写在 reduce()方法中
- Reducetask 进程对每一组相同 k 的<k,v>组调用一次 reduce()方法
- 用户自定义的 Mapper 和 Reducer 都要继承各自的父类
- 整个程序需要一个 Drvier 来进行提交，提交的是一个描述了各种必要信息的 job 对象

####  Wordt Count  示例编写

需求：在一堆给定的文本文件中统计输出每一个单词出现的总次数

-  定义一个 mapper 

```java
//首先要定义四个泛型的类型
//keyin: LongWritable valuein: Text
//keyout: Text valueout:IntWritable
public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable>{
//map 方法的生命周期： 框架每传一行数据就被调用一次
//key : 这一行的起始点在文件中的偏移量
//value: 这一行的内容
@Override
protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException
{
//拿到一行数据转换为 string
String line = value.toString();
//将这一行切分出各个单词
String[] words = line.split(" ");
//遍历数组，输出<单词，1>
for(String word:words){
context.write(new Text(word), new IntWritable(1));
}
}
}    
```

-  定义一个 reducer 

```java
//生命周期：框架每传递进来一个 kv 组，reduce 方法被调用一次
@Override
protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException,
InterruptedException {
//定义一个计数器
int count = 0;
//遍历这一组 kv 的所有 v，累加到 count 中
for(IntWritable value:values){
count += value.get();
}
context.write(key, new IntWritable(count));
}
}
```

- 定义一个主类，用来描述 job  并提交 job

```java
public class WordCountRunner {
//把业务逻辑相关的信息（哪个是 mapper，哪个是 reducer，要处理的数据在哪里，输出的结果放哪里……）描述成一个 job 对象
//把这个描述好的 job 提交给集群去运行
public static void main(String[] args) throws Exception {
Configuration conf = new Configuration();
Job wcjob = Job.getInstance(conf);
//指定我这个 job 所在的 jar 包
//  wcjob.setJar("/home/hadoop/wordcount.jar");
wcjob.setJarByClass(WordCountRunner.class);
wcjob.setMapperClass(WordCountMapper.class);
wcjob.setReducerClass(WordCountReducer.class);
//设置我们的业务逻辑 Mapper 类的输出 key 和 value 的数据类型
wcjob.setMapOutputKeyClass(Text.class);
wcjob.setMapOutputValueClass(IntWritable.class);
//设置我们的业务逻辑 Reducer 类的输出 key 和 value 的数据类型
wcjob.setOutputKeyClass(Text.class);
wcjob.setOutputValueClass(IntWritable.class);
//指定要处理的数据所在的位置
FileInputFormat.setInputPaths(wcjob, "hdfs://hdp-server01:9000/wordcount/data/big.txt");
//指定处理完成之后的结果所保存的位置
FileOutputFormat.setOutputPath(wcjob, new Path("hdfs://hdp-server01:9000/wordcount/output/"));
//向 yarn 集群提交这个 job
boolean res = wcjob.waitForCompletion(true);
System.exit(res?0:1);
}    
```

### MapReduce  程序运行模式

#### 本地运行模式

- mapreduce 程序是被提交给 LocalJobRunner 在本地以单进程的形式运行

- 而处理的数据及输出结果可以在本地文件系统，也可以在 hdfs 上

- 怎样实现本地运行？写一个程序，不要带集群的配置文件

  本质是程序的 conf 中是否有 mapreduce.framework.name=local 以及

  yarn.resourcemanager.hostname 参数

- 本地模式非常便于进行业务逻辑的 debug，只要在 eclipse 中打断点即可

#### 集群运行模式

- 将 mapreduce 程序提交给 yarn 集群，分发到很多的节点上并发执行

- 处理的数据和输出结果应该位于 hdfs 文件系统

- 提交集群的实现步骤：

  将程序打成 JAR 包，然后在集群的任意一个节点上用 hadoop 命令启动

```shell
hadoop jar wordcount.jar cn.itcast.bigdata.mrsimple.WordCountDriver args
```

